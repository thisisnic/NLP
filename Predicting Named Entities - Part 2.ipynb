{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Work in Progress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "training_set = pickle.load(open(\"training_set.p\", \"rb\"))\n",
    "dev_set = pickle.load(open( \"dev_set.p\", \"rb\"))\n",
    "test_set = pickle.load(open( \"test_set.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem here is that the sentences have been already tokenised - spaCy works better with complete sentence for part-of-speech tagging and dependency parsing.  However, let's try it anyway on the individual words and see if we can still improve our model.\n",
    "\n",
    "This will be fairly slow to run seeing as we are calling the language model on every token individually instead of on a per-document basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_with_pos(dataset):\n",
    "    new_dataset = []\n",
    "    for tweet in dataset:\n",
    "        this_tweet = []\n",
    "        for term in tweet:\n",
    "            # if spaCy splits the term into multiple tokens, just use the POS tag for the first one\n",
    "            doc = nlp(term[0])\n",
    "            this_tweet.append((term[0], doc[0].pos_, term[1]))\n",
    "        new_dataset.append(this_tweet)\n",
    "    return new_dataset\n",
    "\n",
    "new_training_set = update_data_with_pos(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just a quick bit of sense-checking to make sure it looks OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1436\n",
      "1436\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set))\n",
    "print(len(new_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SeanKingston', 'O'), ('Hey', 'O'), ('sweety', 'O'), ('have', 'O'), ('a', 'O'), ('great', 'O'), (',', 'O'), ('relaxing', 'O'), ('time', 'O'), (':)', 'O'), ('Hugs', 'O'), ('to', 'O'), ('you', 'O'), ('!', 'O')]\n",
      "\n",
      "\n",
      "[('@SeanKingston', 'PROPN', 'O'), ('Hey', 'INTJ', 'O'), ('sweety', 'NOUN', 'O'), ('have', 'VERB', 'O'), ('a', 'DET', 'O'), ('great', 'ADJ', 'O'), (',', 'PUNCT', 'O'), ('relaxing', 'VERB', 'O'), ('time', 'NOUN', 'O'), (':)', 'PUNCT', 'O'), ('Hugs', 'NOUN', 'O'), ('to', 'PART', 'O'), ('you', 'PRON', 'O'), ('!', 'PUNCT', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(training_set[42])\n",
    "print(\"\\n\")\n",
    "print(new_training_set[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
